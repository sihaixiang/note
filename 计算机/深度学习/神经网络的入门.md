### 网络、层、损失函数与优化器之间的关系

![jpg](https://s2.loli.net/2022/01/20/LREf1wsTXbhU9ga.jpg)

#### 层

- 简单向量数据——2D张量——密集连接层
- 序列数据——3D张量——循环层
- 图像数据——4D张量——二维卷积层

##### 损失函数

```python
#例：单一损失函数
from keras import optimizers

model.compile(optimizers = optimizers(Lr = 0.001),
              loss = 'mse',
              metrics = ['accuracy'])
```

```python
#学习过程通过fit()方法输入数据的numpy数组（和对应的目标数据）传入模型
model.fit(input_tensor, target_tensor, batch_size = 128, epochs = 10)
```



### 二分类问题（电影评论分类）

##### 加载IMDB数据集

```python
#加载IMDB数据集
from keras.datasets import imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(
    num_words = 10000
)

#10000的意思是仅保留训练数据中前10000个最常见出现的单词

train_data[0]

train_labels[0]
#train_data和test_data都是评论组成的列表，每条评论又是单词索引组成的列表
#train_labels和testlabels都是0和1组成的列表，0表示负面，1表示正面

max([max(sequence) for sequence in train_data])
#单词索引都不会超过10000个

word_index = imdb.get_word_index()
#word_index是一个将单词映射为整数索引的字典

reverse_word_index = dict(
    [(value, key) for (key, value) in word_index.items()])
    #键值颠倒，将整数索引映射为单词
decoded_review = ' '.join(
    [reverse_word_index.get(i - 3, '?') for i in train_data[0]])
```

##### 准备数据

```python
#准备数据
import numpy as np
def vectorize_sequences(sequences, dimension = 10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

x_train = vectorize_sequences(train_data)
y_text = vectorize_sequences(test_data)
```



### 多分类问题

### 标量回归问题

